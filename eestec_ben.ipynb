{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bf35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data set:\n",
    "#necessary imports\n",
    "import torch\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Load data set\n",
    "dataset = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08826190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torchtext \n",
    "from torchtext.data import Field, TabularDataset\n",
    "#Define pre processing for the statement column \n",
    "#Statement = Field(sequential =  True, lower = True, tokenize = \"spacy\", stop_words = \"english\", batch_first = True)\n",
    "#Define the Score field \n",
    "#Label_False = Field(sequential = False, use_vocab = False, is_Target = True, batch_first = True, dtype = torch.int16)\n",
    "#Label_Barely_True =  Field(sequential = False, use_vocab = False, is_Target = True, batch_first = True, dtype = torch.int16)\n",
    "#Label_Half_True =  Field(sequential = False, use_vocab = False, is_Target = True, batch_first = True, dtype = torch.int16)\n",
    "#Label_Mostly_True =  Field(sequential = False, use_vocab = False, is_Target = True, batch_first = True, dtype = torch.int16)\n",
    "#Label_Pants_Fire =  Field(sequential = False, use_vocab = False, is_Target = True, batch_first = True, dtype = torch.int16)\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for numerical features\n",
    "import numpy as np\n",
    "\n",
    "File_Path = 'eestec_hackathon_2025_train.tsv'\n",
    "Text_Column = 'Statement'\n",
    "Numerical_Columns = [\n",
    "    'Credit History: barely-true', 'Credit History: false',\n",
    "    'Credit History: half-true', 'Credit History: mostly-true',\n",
    "    'Credit History: pants-fire'\n",
    "]\n",
    "Label_Column = 'Label'\n",
    "\n",
    "\n",
    "#Batch size \n",
    "Batch_Size = 64\n",
    "\n",
    "#Read file  \n",
    "Data_File = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "\n",
    "#Split up the data in the validation and training set\n",
    "Train_df, Val_df = train_test_split(Data_File, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the text \n",
    "natural_language_processor = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #Lemmatize every token in set, after tokenizaziotn \n",
    "    return [token.lemma_.lower() for token in natural_language_processor(str(text)) if not token.is_punct and not token.is_space]\n",
    "\n",
    "#Now we build the vocabulary \n",
    "word_count = Counter()\n",
    "for statement in Train_df[Text_Column]:\n",
    "    word_count.update(tokenize_text(statement))\n",
    "\n",
    "# add PAD token to make same length and UNK(unkown for words not in the vocab)\n",
    "Max_Vocab_Size = 10000\n",
    "Max_Length = 50  \n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, count) in enumerate(word_count.most_common(Max_Vocab_Size-2)): # -2 for pad and unk\n",
    "    vocab[word] = i + 2\n",
    "\n",
    "actual_Vocab_Size = len[vocab]\n",
    "\n",
    "#Numericalize and pad/truncuate text\n",
    "def numericalize_pad_text(text, vocab_map, max_len):\n",
    "    tokens = tokenize_text(text)\n",
    "    numericalized = [vocab_map.get(token, vocab_map[UNK_TOKEN]) for token in tokens]\n",
    "    if len(numericalized) < max_len:\n",
    "        # Pad with PAD_TOKEN's index\n",
    "        numericalized.extend([vocab_map[PAD_TOKEN]] * (max_len - len(numericalized)))\n",
    "    else:\n",
    "        # Truncate\n",
    "        numericalized = numericalized[:max_len]\n",
    "    return numericalized\n",
    "\n",
    "# Apply to Statement column\n",
    "Train_df['statement_numerical'] = Train_df[Text_Column].apply(lambda x: numericalize_pad_text(x, vocab, Max_Length))\n",
    "Val_df['statement_numerical'] = Val_df[Text_Column].apply(lambda x: numericalize_pad_text(x, vocab, Max_Length))\n",
    "\n",
    "# Later maybe use scalar?\n",
    "# scaler = StandardScaler()\n",
    "# Train_df[Numerical_Columns] = scaler.fit_transform(Train_df[Numerical_Columns])\n",
    "# Val_df[Numerical_Columns] = scaler.transform(Val_df[Numerical_Columns]) # Use transform for validation\n",
    "\n",
    "# Mapping the labels to numbers\n",
    "unique_labels = Train_df[Label_Column].astype(str).unique()\n",
    "label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Use mapping for data \n",
    "Train_df['label_idx'] = Train_df[Label_Column].astype(str).map(label_to_idx)\n",
    "Val_df['label_idx'] = Val_df[Label_Column].astype(str).map(label_to_idx)\n",
    "\n",
    "# Verify pleasefgxh cgjvb\n",
    "class TextNumericalDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_col_numerical, numerical_cols_list, label_col_idx):\n",
    "        self.texts = torch.tensor(list(dataframe[text_col_numerical].values), dtype=torch.long)\n",
    "        self.numerical_features = torch.tensor(dataframe[numerical_cols_list].values, dtype=torch.int16)\n",
    "        self.labels = torch.tensor(dataframe[label_col_idx].values, dtype=torch.int8) # Assuming classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'numerical': self.numerical_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextNumericalDataset(Train_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "val_dataset = TextNumericalDataset(Val_df, 'statement_numerical', Numerical_Columns, 'label_idx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
