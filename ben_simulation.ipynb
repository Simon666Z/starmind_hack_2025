{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "067bb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "from torch import nn, tensor, save, load\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f940d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "File_Path = 'eestec_hackathon_2025_train.tsv'\n",
    "Statement_Column = 'Statement'\n",
    "Numerical_Columns = [\n",
    "    'Credit History: barely-true', 'Credit History: false',\n",
    "    'Credit History: half-true', 'Credit History: mostly-true',\n",
    "    'Credit History: pants-fire'\n",
    "]\n",
    "Label_Column = 'Label'\n",
    "\n",
    "# batch size \n",
    "Batch_Size = 64\n",
    "\n",
    "# read file  \n",
    "Data_File = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "\n",
    "# preprocess the text \n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "natural_language_processor = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #Lemmatize every token in set, after tokenizaziotn \n",
    "    return [token.lemma_.lower() for token in natural_language_processor(str(text)) if not token.is_punct and not token.is_space and not token.is_stop and not token.is_alpha]\n",
    "\n",
    "# build the vocabulary\n",
    "word_count = Counter()\n",
    "for statement in Data_File[Statement_Column]:\n",
    "    word_count.update(tokenize_text(statement))\n",
    "\n",
    "# add PAD token to make same length and UNK (unkown for words not in the vocab)\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 50\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, count) in enumerate(word_count.most_common(MAX_VOCAB_SIZE- 2)): # -2 for pad and unk\n",
    "    vocab[word] = i + 2\n",
    "\n",
    "actual_vocab_size = len(vocab)\n",
    "\n",
    "# numericalize and pad/truncuate text\n",
    "def numericalize_pad_text(text, vocab_map, max_length):\n",
    "    tokens = tokenize_text(text)\n",
    "    numericalized = [vocab_map.get(token, vocab_map[UNK_TOKEN]) for token in tokens]\n",
    "    if len(numericalized) < max_length:\n",
    "        # Pad with PAD_TOKEN's index\n",
    "        numericalized.extend([vocab_map[PAD_TOKEN]] * (max_length - len(numericalized)))\n",
    "    else:\n",
    "        # Truncate\n",
    "        numericalized = numericalized[:max_length]\n",
    "    return numericalized\n",
    "\n",
    "# numericalize the statements into a new 'statement_numerical' column\n",
    "Data_File['statement_numerical'] = Data_File[Statement_Column].apply(lambda x: numericalize_pad_text(x, vocab, MAX_LENGTH))\n",
    "\n",
    "# map the labels to numerical values\n",
    "unique_labels = Data_File[Label_Column].astype(str).unique()\n",
    "label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# use mapping for data in new 'label_idx' column\n",
    "Data_File['label_idx'] = Data_File[Label_Column].astype(str).map(label_to_idx)\n",
    "\n",
    "for col in Numerical_Columns:\n",
    "    Data_File[col] = pd.to_numeric(Data_File[col], errors='coerce')\n",
    "    \n",
    "    if Data_File[col].isnull().any():\n",
    "        Data_File[col] = Data_File[col].fillna(0)\n",
    "\n",
    "# scale the credit history values\n",
    "# scaler = StandardScaler()\n",
    "# Data_File[Numerical_Columns] = scaler.fit_transform(Data_File[Numerical_Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f67b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names: Shuming Zhao, Arthur Hennig, Ben Kracht\n",
    "\n",
    "\n",
    "class LieDetectorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_lstm, num_numerical_features, hidden_dim_fc, output_dim, pad_idx):\n",
    "        super(LieDetectorModel, self).__init__()\n",
    "        # Embedding layer for text\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # LSTM layer to process sequential text data\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_lstm, batch_first=True, num_layers=1, bidirectional=True) # Using bidirectional LSTM\n",
    "      \n",
    "        lstm_output_features = hidden_dim_lstm * 2 \n",
    "\n",
    "        self.fc1 = nn.Linear(lstm_output_features + num_numerical_features, hidden_dim_fc)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        self.fc2 = nn.Linear(hidden_dim_fc, output_dim)\n",
    "\n",
    "    def forward(self, text_data, numerical_data):\n",
    "        embedded = self.embedding(text_data)     \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        combined_features = torch.cat((hidden_combined, numerical_data), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        # logits shape: (batch_size, output_dim)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, model_save_path='model_weights.pt'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (optim.Optimizer): The optimizer to use (e.g., Adam).\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        device (torch.device): The device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    previous_best_model_path = None \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #Go to training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            texts = batch['text'].to(device)\n",
    "            numerical_feats = batch['numerical'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(texts, numerical_feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * texts.size(0)\n",
    "\n",
    "            # calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples_train += labels.size(0)\n",
    "            correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = correct_predictions_train / total_samples_train\n",
    "\n",
    "        #Go to validation\n",
    "        model.eval() \n",
    "        running_val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "        #No grad for validation\n",
    "        with torch.no_grad(): \n",
    "            for batch in val_loader:\n",
    "                texts = batch['text'].to(device)\n",
    "                numerical_feats = batch['numerical'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(texts, numerical_feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * texts.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples_val += labels.size(0)\n",
    "                correct_predictions_val += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_acc_val = correct_predictions_val / total_samples_val\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc_train:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_val:.4f}\")\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {epoch_val_loss:.4f}.\")\n",
    "\n",
    "            # Delete the previous best model file if it exists\n",
    "            if previous_best_model_path and os.path.exists(previous_best_model_path):\n",
    "                os.remove(previous_best_model_path)\n",
    "                print(f\"Deleted old checkpoint: {previous_best_model_path}\")\n",
    "\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_val_acc = epoch_acc_val\n",
    "\n",
    "            # Construct a dynamic filename that includes the epoch and validation loss for clarity\n",
    "            # This ensures unique filenames if you decide to keep top-k, but we're keeping only the best here\n",
    "            current_model_save_path = f\"best_model_epoch_{epoch+1}_val_loss_{best_val_loss:.4f}.pt\"\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'vocab': vocab,\n",
    "                'label_to_idx': label_to_idx,\n",
    "                'idx_to_label': idx_to_label,\n",
    "                'num_classes': num_classes,\n",
    "                'embedding_dim': model.embedding.embedding_dim,\n",
    "                'hidden_dim_lstm': model.lstm.hidden_size,\n",
    "                'num_numerical_features': model.fc1.in_features - (model.lstm.hidden_size * 2),\n",
    "                'hidden_dim_fc': model.fc1.out_features,\n",
    "                'pad_idx': model.embedding.padding_idx\n",
    "            }\n",
    "            torch.save(checkpoint, current_model_save_path)\n",
    "            print(f\"New best checkpoint saved to {current_model_save_path}\")\n",
    "            \n",
    "            # Update the path to the currently best model\n",
    "            previous_best_model_path = current_model_save_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2462f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of numerical features being used: 5\n",
      "Starting training...\n",
      "Epoch [1/42] | Train Loss: 2.1434 | Train Acc: 0.2230 | Val Loss: 1.7463 | Val Acc: 0.3116\n",
      "Validation loss improved from inf to 1.7463.\n",
      "New best checkpoint saved to best_model_epoch_1_val_loss_1.7463.pt\n",
      "Epoch [2/42] | Train Loss: 1.7995 | Train Acc: 0.2621 | Val Loss: 1.6962 | Val Acc: 0.3535\n",
      "Validation loss improved from 1.7463 to 1.6962.\n",
      "Deleted old checkpoint: best_model_epoch_1_val_loss_1.7463.pt\n",
      "New best checkpoint saved to best_model_epoch_2_val_loss_1.6962.pt\n",
      "Epoch [3/42] | Train Loss: 1.7202 | Train Acc: 0.2839 | Val Loss: 1.6779 | Val Acc: 0.3501\n",
      "Validation loss improved from 1.6962 to 1.6779.\n",
      "Deleted old checkpoint: best_model_epoch_2_val_loss_1.6962.pt\n",
      "New best checkpoint saved to best_model_epoch_3_val_loss_1.6779.pt\n",
      "Epoch [4/42] | Train Loss: 1.6785 | Train Acc: 0.3056 | Val Loss: 1.6624 | Val Acc: 0.4016\n",
      "Validation loss improved from 1.6779 to 1.6624.\n",
      "Deleted old checkpoint: best_model_epoch_3_val_loss_1.6779.pt\n",
      "New best checkpoint saved to best_model_epoch_4_val_loss_1.6624.pt\n",
      "Epoch [5/42] | Train Loss: 1.6513 | Train Acc: 0.3161 | Val Loss: 1.6464 | Val Acc: 0.3652\n",
      "Validation loss improved from 1.6624 to 1.6464.\n",
      "Deleted old checkpoint: best_model_epoch_4_val_loss_1.6624.pt\n",
      "New best checkpoint saved to best_model_epoch_5_val_loss_1.6464.pt\n",
      "Epoch [6/42] | Train Loss: 1.6362 | Train Acc: 0.3290 | Val Loss: 1.6307 | Val Acc: 0.3570\n",
      "Validation loss improved from 1.6464 to 1.6307.\n",
      "Deleted old checkpoint: best_model_epoch_5_val_loss_1.6464.pt\n",
      "New best checkpoint saved to best_model_epoch_6_val_loss_1.6307.pt\n",
      "Epoch [7/42] | Train Loss: 1.6172 | Train Acc: 0.3459 | Val Loss: 1.6260 | Val Acc: 0.4037\n",
      "Validation loss improved from 1.6307 to 1.6260.\n",
      "Deleted old checkpoint: best_model_epoch_6_val_loss_1.6307.pt\n",
      "New best checkpoint saved to best_model_epoch_7_val_loss_1.6260.pt\n",
      "Epoch [8/42] | Train Loss: 1.6027 | Train Acc: 0.3508 | Val Loss: 1.6094 | Val Acc: 0.3687\n",
      "Validation loss improved from 1.6260 to 1.6094.\n",
      "Deleted old checkpoint: best_model_epoch_7_val_loss_1.6260.pt\n",
      "New best checkpoint saved to best_model_epoch_8_val_loss_1.6094.pt\n",
      "Epoch [9/42] | Train Loss: 1.5882 | Train Acc: 0.3632 | Val Loss: 1.5989 | Val Acc: 0.4055\n",
      "Validation loss improved from 1.6094 to 1.5989.\n",
      "Deleted old checkpoint: best_model_epoch_8_val_loss_1.6094.pt\n",
      "New best checkpoint saved to best_model_epoch_9_val_loss_1.5989.pt\n",
      "Epoch [10/42] | Train Loss: 1.5731 | Train Acc: 0.3751 | Val Loss: 1.5851 | Val Acc: 0.3821\n",
      "Validation loss improved from 1.5989 to 1.5851.\n",
      "Deleted old checkpoint: best_model_epoch_9_val_loss_1.5989.pt\n",
      "New best checkpoint saved to best_model_epoch_10_val_loss_1.5851.pt\n",
      "Epoch [11/42] | Train Loss: 1.5568 | Train Acc: 0.3777 | Val Loss: 1.5827 | Val Acc: 0.3964\n",
      "Validation loss improved from 1.5851 to 1.5827.\n",
      "Deleted old checkpoint: best_model_epoch_10_val_loss_1.5851.pt\n",
      "New best checkpoint saved to best_model_epoch_11_val_loss_1.5827.pt\n",
      "Epoch [12/42] | Train Loss: 1.5375 | Train Acc: 0.3775 | Val Loss: 1.5689 | Val Acc: 0.4124\n",
      "Validation loss improved from 1.5827 to 1.5689.\n",
      "Deleted old checkpoint: best_model_epoch_11_val_loss_1.5827.pt\n",
      "New best checkpoint saved to best_model_epoch_12_val_loss_1.5689.pt\n",
      "Epoch [13/42] | Train Loss: 1.5258 | Train Acc: 0.3909 | Val Loss: 1.5581 | Val Acc: 0.3916\n",
      "Validation loss improved from 1.5689 to 1.5581.\n",
      "Deleted old checkpoint: best_model_epoch_12_val_loss_1.5689.pt\n",
      "New best checkpoint saved to best_model_epoch_13_val_loss_1.5581.pt\n",
      "Epoch [14/42] | Train Loss: 1.5066 | Train Acc: 0.4010 | Val Loss: 1.5440 | Val Acc: 0.4059\n",
      "Validation loss improved from 1.5581 to 1.5440.\n",
      "Deleted old checkpoint: best_model_epoch_13_val_loss_1.5581.pt\n",
      "New best checkpoint saved to best_model_epoch_14_val_loss_1.5440.pt\n",
      "Epoch [15/42] | Train Loss: 1.4961 | Train Acc: 0.4053 | Val Loss: 1.5387 | Val Acc: 0.3972\n",
      "Validation loss improved from 1.5440 to 1.5387.\n",
      "Deleted old checkpoint: best_model_epoch_14_val_loss_1.5440.pt\n",
      "New best checkpoint saved to best_model_epoch_15_val_loss_1.5387.pt\n",
      "Epoch [16/42] | Train Loss: 1.4788 | Train Acc: 0.4154 | Val Loss: 1.5296 | Val Acc: 0.4102\n",
      "Validation loss improved from 1.5387 to 1.5296.\n",
      "Deleted old checkpoint: best_model_epoch_15_val_loss_1.5387.pt\n",
      "New best checkpoint saved to best_model_epoch_16_val_loss_1.5296.pt\n",
      "Epoch [17/42] | Train Loss: 1.4672 | Train Acc: 0.4194 | Val Loss: 1.5153 | Val Acc: 0.4150\n",
      "Validation loss improved from 1.5296 to 1.5153.\n",
      "Deleted old checkpoint: best_model_epoch_16_val_loss_1.5296.pt\n",
      "New best checkpoint saved to best_model_epoch_17_val_loss_1.5153.pt\n",
      "Epoch [18/42] | Train Loss: 1.4511 | Train Acc: 0.4279 | Val Loss: 1.5080 | Val Acc: 0.4119\n",
      "Validation loss improved from 1.5153 to 1.5080.\n",
      "Deleted old checkpoint: best_model_epoch_17_val_loss_1.5153.pt\n",
      "New best checkpoint saved to best_model_epoch_18_val_loss_1.5080.pt\n",
      "Epoch [19/42] | Train Loss: 1.4356 | Train Acc: 0.4308 | Val Loss: 1.4982 | Val Acc: 0.4176\n",
      "Validation loss improved from 1.5080 to 1.4982.\n",
      "Deleted old checkpoint: best_model_epoch_18_val_loss_1.5080.pt\n",
      "New best checkpoint saved to best_model_epoch_19_val_loss_1.4982.pt\n",
      "Epoch [20/42] | Train Loss: 1.4275 | Train Acc: 0.4363 | Val Loss: 1.5008 | Val Acc: 0.4098\n",
      "Epoch [21/42] | Train Loss: 1.4132 | Train Acc: 0.4420 | Val Loss: 1.4871 | Val Acc: 0.4271\n",
      "Validation loss improved from 1.4982 to 1.4871.\n",
      "Deleted old checkpoint: best_model_epoch_19_val_loss_1.4982.pt\n",
      "New best checkpoint saved to best_model_epoch_21_val_loss_1.4871.pt\n",
      "Epoch [22/42] | Train Loss: 1.4041 | Train Acc: 0.4460 | Val Loss: 1.4806 | Val Acc: 0.4275\n",
      "Validation loss improved from 1.4871 to 1.4806.\n",
      "Deleted old checkpoint: best_model_epoch_21_val_loss_1.4871.pt\n",
      "New best checkpoint saved to best_model_epoch_22_val_loss_1.4806.pt\n",
      "Epoch [23/42] | Train Loss: 1.3872 | Train Acc: 0.4513 | Val Loss: 1.4743 | Val Acc: 0.4236\n",
      "Validation loss improved from 1.4806 to 1.4743.\n",
      "Deleted old checkpoint: best_model_epoch_22_val_loss_1.4806.pt\n",
      "New best checkpoint saved to best_model_epoch_23_val_loss_1.4743.pt\n",
      "Epoch [24/42] | Train Loss: 1.3726 | Train Acc: 0.4588 | Val Loss: 1.4684 | Val Acc: 0.4314\n",
      "Validation loss improved from 1.4743 to 1.4684.\n",
      "Deleted old checkpoint: best_model_epoch_23_val_loss_1.4743.pt\n",
      "New best checkpoint saved to best_model_epoch_24_val_loss_1.4684.pt\n",
      "Epoch [25/42] | Train Loss: 1.3662 | Train Acc: 0.4569 | Val Loss: 1.4629 | Val Acc: 0.4210\n",
      "Validation loss improved from 1.4684 to 1.4629.\n",
      "Deleted old checkpoint: best_model_epoch_24_val_loss_1.4684.pt\n",
      "New best checkpoint saved to best_model_epoch_25_val_loss_1.4629.pt\n",
      "Epoch [26/42] | Train Loss: 1.3507 | Train Acc: 0.4623 | Val Loss: 1.4589 | Val Acc: 0.4197\n",
      "Validation loss improved from 1.4629 to 1.4589.\n",
      "Deleted old checkpoint: best_model_epoch_25_val_loss_1.4629.pt\n",
      "New best checkpoint saved to best_model_epoch_26_val_loss_1.4589.pt\n",
      "Epoch [27/42] | Train Loss: 1.3413 | Train Acc: 0.4709 | Val Loss: 1.4614 | Val Acc: 0.4154\n",
      "Epoch [28/42] | Train Loss: 1.3395 | Train Acc: 0.4705 | Val Loss: 1.4474 | Val Acc: 0.4288\n",
      "Validation loss improved from 1.4589 to 1.4474.\n",
      "Deleted old checkpoint: best_model_epoch_26_val_loss_1.4589.pt\n",
      "New best checkpoint saved to best_model_epoch_28_val_loss_1.4474.pt\n",
      "Epoch [29/42] | Train Loss: 1.3171 | Train Acc: 0.4742 | Val Loss: 1.4429 | Val Acc: 0.4202\n",
      "Validation loss improved from 1.4474 to 1.4429.\n",
      "Deleted old checkpoint: best_model_epoch_28_val_loss_1.4474.pt\n",
      "New best checkpoint saved to best_model_epoch_29_val_loss_1.4429.pt\n",
      "Epoch [30/42] | Train Loss: 1.3143 | Train Acc: 0.4776 | Val Loss: 1.4421 | Val Acc: 0.4215\n",
      "Validation loss improved from 1.4429 to 1.4421.\n",
      "Deleted old checkpoint: best_model_epoch_29_val_loss_1.4429.pt\n",
      "New best checkpoint saved to best_model_epoch_30_val_loss_1.4421.pt\n",
      "Epoch [31/42] | Train Loss: 1.3040 | Train Acc: 0.4774 | Val Loss: 1.4383 | Val Acc: 0.4171\n",
      "Validation loss improved from 1.4421 to 1.4383.\n",
      "Deleted old checkpoint: best_model_epoch_30_val_loss_1.4421.pt\n",
      "New best checkpoint saved to best_model_epoch_31_val_loss_1.4383.pt\n",
      "Epoch [32/42] | Train Loss: 1.2931 | Train Acc: 0.4873 | Val Loss: 1.4387 | Val Acc: 0.4167\n",
      "Epoch [33/42] | Train Loss: 1.2864 | Train Acc: 0.4872 | Val Loss: 1.4286 | Val Acc: 0.4258\n",
      "Validation loss improved from 1.4383 to 1.4286.\n",
      "Deleted old checkpoint: best_model_epoch_31_val_loss_1.4383.pt\n",
      "New best checkpoint saved to best_model_epoch_33_val_loss_1.4286.pt\n",
      "Epoch [34/42] | Train Loss: 1.2734 | Train Acc: 0.4889 | Val Loss: 1.4345 | Val Acc: 0.4176\n",
      "Epoch [35/42] | Train Loss: 1.2700 | Train Acc: 0.4866 | Val Loss: 1.4332 | Val Acc: 0.4206\n",
      "Epoch [36/42] | Train Loss: 1.2607 | Train Acc: 0.4936 | Val Loss: 1.4290 | Val Acc: 0.4215\n",
      "Epoch [37/42] | Train Loss: 1.2564 | Train Acc: 0.4951 | Val Loss: 1.4322 | Val Acc: 0.4158\n",
      "Epoch [38/42] | Train Loss: 1.2455 | Train Acc: 0.4976 | Val Loss: 1.4331 | Val Acc: 0.4189\n",
      "Epoch [39/42] | Train Loss: 1.2373 | Train Acc: 0.5094 | Val Loss: 1.4285 | Val Acc: 0.4258\n",
      "Validation loss improved from 1.4286 to 1.4285.\n",
      "Deleted old checkpoint: best_model_epoch_33_val_loss_1.4286.pt\n",
      "New best checkpoint saved to best_model_epoch_39_val_loss_1.4285.pt\n",
      "Epoch [40/42] | Train Loss: 1.2284 | Train Acc: 0.5023 | Val Loss: 1.4345 | Val Acc: 0.4102\n",
      "Epoch [41/42] | Train Loss: 1.2219 | Train Acc: 0.5083 | Val Loss: 1.4325 | Val Acc: 0.4102\n",
      "Epoch [42/42] | Train Loss: 1.2177 | Train Acc: 0.5124 | Val Loss: 1.4309 | Val Acc: 0.4206\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 42\n",
    "\n",
    "# split up the data in the validation and training set\n",
    "Train_df, Val_df = train_test_split(Data_File, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset for statements (processing textual data into numerical data)\n",
    "class TextNumericalDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_col_numerical, numerical_cols_list, label_col_idx):\n",
    "        self.texts = torch.tensor(list(dataframe[text_col_numerical].values), dtype=torch.long)\n",
    "        self.numerical_features = torch.tensor(dataframe[numerical_cols_list].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataframe[label_col_idx].values, dtype=torch.long) # Assuming classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'numerical': self.numerical_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextNumericalDataset(Train_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "val_dataset = TextNumericalDataset(Val_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "\n",
    "# data loader for both datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Batch_Size, shuffle=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # determine device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    num_numerical_features = train_dataset.numerical_features.shape[1]\n",
    "    print(f\"Number of numerical features being used: {num_numerical_features}\")\n",
    "    pad_idx = vocab[PAD_TOKEN]\n",
    "    model = LieDetectorModel(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=75,\n",
    "        hidden_dim_lstm=64,\n",
    "        num_numerical_features=num_numerical_features,\n",
    "        hidden_dim_fc=128,\n",
    "        output_dim=num_classes,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0003)  # Using AdamW optimizer\n",
    "    # CrossEntropyLoss is suitable for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # start training\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49d5d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lie Detector Function Cell\n",
    "def lie_detector(statement, \n",
    "                 subjects, \n",
    "                 speaker_name, \n",
    "                 speaker_title, \n",
    "                 state, \n",
    "                 party_affiliation, \n",
    "                 history_barely_true, \n",
    "                 history_false, \n",
    "                 history_half_true, \n",
    "                 history_mostly_true, \n",
    "                 history_pants_fire, \n",
    "                 context_location):\n",
    "    \n",
    "    model = LieDetectorModel(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=75,\n",
    "        hidden_dim_lstm=64,\n",
    "        num_numerical_features=num_numerical_features,\n",
    "        hidden_dim_fc=128,\n",
    "        output_dim=num_classes,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "    \n",
    "    with open('best_model_epoch_39_val_loss_1.4285.pt', 'rb') as save_file:\n",
    "     checkpoint = load(save_file, weights_only=False) # Load the full checkpoint dictionary\n",
    "     model.load_state_dict(checkpoint['model_state_dict']) # Extract model's state_dict\n",
    "    return ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true'][model(numericalize_pad_text(statement, vocab, MAX_VOCAB_SIZE), torch.tensor([history_barely_true, history_false, history_half_true, history_mostly_true, history_pants_fire])).item()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bee7b1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m Data_File = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33meestec_hackathon_2025_train.tsv\u001b[39m\u001b[33m'\u001b[39m ,sep = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m,names=[\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStatement\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubjects\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSpeaker Name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSpeaker Title\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mState\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mParty Affiliation\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCredit History: barely-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCredit History: false\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCredit History: half-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCredit History: mostly-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCredit History: pants-fire\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mContext/Location\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m a, b, c, d, e, f, g, h, i, j, k, l = Data_File.iloc[\u001b[32m0\u001b[39m].values[\u001b[32m2\u001b[39m:]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mlie_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mlie_detector\u001b[39m\u001b[34m(statement, subjects, speaker_name, speaker_title, state, party_affiliation, history_barely_true, history_false, history_half_true, history_mostly_true, history_pants_fire, context_location)\u001b[39m\n\u001b[32m     26\u001b[39m  checkpoint = load(save_file, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Load the full checkpoint dictionary\u001b[39;00m\n\u001b[32m     27\u001b[39m  model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m# Extract model's state_dict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mpants-fire\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbarely-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhalf-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmostly-true\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m][model(numericalize_pad_text(statement, vocab, MAX_VOCAB_SIZE), \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhistory_barely_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_false\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_half_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_mostly_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_pants_fire\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m).item()[\u001b[32m1\u001b[39m]]\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# read file  \n",
    "Data_File = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "a, b, c, d, e, f, g, h, i, j, k, l = Data_File.iloc[0].values[2:]\n",
    "lie_detector(a, b, c, d, e, f, g, h, i, j, k, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
