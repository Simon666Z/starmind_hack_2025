{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple Neural Network Model Definition ---\n",
    "class SimpleTextNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_lstm, num_numerical_features, hidden_dim_fc, output_dim, pad_idx):\n",
    "        super(SimpleTextNN, self).__init__()\n",
    "        # Embedding layer for text\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # LSTM layer to process sequential text data\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_lstm, batch_first=True, num_layers=1, bidirectional=True) # Using bidirectional LSTM\n",
    "        \n",
    "        # Calculate the input size for the first fully connected layer\n",
    "        # LSTM output is (batch_size, seq_len, 2 * hidden_dim_lstm) because bidirectional=True\n",
    "        # We'll take the final hidden state of the LSTM (or an aggregation like mean/max pooling)\n",
    "        # For simplicity, let's use the concatenation of the final forward and backward hidden states\n",
    "        # The hidden state shape is (num_layers * num_directions, batch_size, hidden_dim_lstm)\n",
    "        # So, the output from LSTM to be used will be 2 * hidden_dim_lstm\n",
    "        lstm_output_features = hidden_dim_lstm * 2 # For bidirectional LSTM\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Input to fc1 will be the concatenated LSTM output and numerical features\n",
    "        self.fc1 = nn.Linear(lstm_output_features + num_numerical_features, hidden_dim_fc)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(hidden_dim_fc, output_dim) # Output layer\n",
    "\n",
    "    def forward(self, text_data, numerical_data):\n",
    "        # text_data shape: (batch_size, seq_len)\n",
    "        # numerical_data shape: (batch_size, num_numerical_features)\n",
    "\n",
    "        embedded = self.embedding(text_data)\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Pass embedded text through LSTM\n",
    "        # outputs shape: (batch_size, seq_len, hidden_dim_lstm * 2)\n",
    "        # hidden shape: (num_layers * 2, batch_size, hidden_dim_lstm)\n",
    "        # cell shape: (num_layers * 2, batch_size, hidden_dim_lstm)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # We can use the final hidden state.\n",
    "        # Concatenate the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden states\n",
    "        hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden_combined shape: (batch_size, hidden_dim_lstm * 2)\n",
    "\n",
    "        # Concatenate LSTM output (text features) with numerical features\n",
    "        # Ensure numerical_data is on the same device and has the correct shape\n",
    "        combined_features = torch.cat((hidden_combined, numerical_data), dim=1)\n",
    "        # combined_features shape: (batch_size, (hidden_dim_lstm * 2) + num_numerical_features)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        # logits shape: (batch_size, output_dim)\n",
    "        return logits\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (optim.Optimizer): The optimizer to use (e.g., Adam).\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        device (torch.device): The device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device) # Move model to the specified device\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            texts = batch['text'].to(device)\n",
    "            numerical_feats = batch['numerical'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(texts, numerical_feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * texts.size(0) # Accumulate loss\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples_train += labels.size(0)\n",
    "            correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = correct_predictions_train / total_samples_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "        with torch.no_grad(): # No gradients needed for validation\n",
    "            for batch in val_loader:\n",
    "                texts = batch['text'].to(device)\n",
    "                numerical_feats = batch['numerical'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(texts, numerical_feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * texts.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples_val += labels.size(0)\n",
    "                correct_predictions_val += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_acc_val = correct_predictions_val / total_samples_val\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc_train:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_val:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Determine device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    # Ensure num_numerical_features matches the number of columns in Numerical_Columns\n",
    "    # that are actually present and used in the dataset.\n",
    "    # The TextNumericalDataset class already handles filtering Numerical_Columns.\n",
    "    # So, we can get the correct count from the dataset's numerical_features shape.\n",
    "    if len(train_dataset.numerical_features.shape) > 1:\n",
    "        num_numerical_features = train_dataset.numerical_features.shape[1]\n",
    "    else: # Handle case where there might be no numerical features or it's 1D\n",
    "        num_numerical_features = 0 if train_dataset.numerical_features.nelement() == 0 else 1\n",
    "        if train_dataset.numerical_features.nelement() > 0 and len(train_dataset.numerical_features.shape) == 1:\n",
    "             # If it's 1D but not empty, it means each sample has 1 numerical feature.\n",
    "             # This might happen if Numerical_Columns has only one valid column.\n",
    "             # However, our current setup expects numerical_features to be 2D (batch_size, num_features).\n",
    "             # If there's truly only one feature, ensure it's shaped as (batch_size, 1) in the Dataset.\n",
    "             # For now, let's assume if it's 1D and not empty, it's an error in data prep or a single feature.\n",
    "             # The .values in Dataset creation should make it 2D if pandas df has multiple columns.\n",
    "             # If Numerical_Columns is empty, .values on an empty list of columns might be tricky.\n",
    "             # The TextNumericalDataset handles this by using valid_numerical_cols.\n",
    "             # So, if valid_numerical_cols is empty, numerical_features will be empty.\n",
    "             pass\n",
    "\n",
    "\n",
    "    print(f\"Number of numerical features being used: {num_numerical_features}\")\n",
    "\n",
    "\n",
    "    pad_idx = vocab[PAD_TOKEN]\n",
    "    model = SimpleTextNN(\n",
    "        vocab_size=actual_Vocab_Size,\n",
    "        embedding_dim=Embedding_Dim,\n",
    "        hidden_dim_lstm=Hidden_Dim_LSTM,\n",
    "        num_numerical_features=num_numerical_features,\n",
    "        hidden_dim_fc=Hidden_Dim_FC,\n",
    "        output_dim=num_classes,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Learning_Rate)\n",
    "    # CrossEntropyLoss is suitable for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, Num_Epochs, device)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
