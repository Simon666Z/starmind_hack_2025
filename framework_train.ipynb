{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3278d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch import nn, tensor, save, load\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e53f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "File_Path = 'eestec_hackathon_2025_train.tsv'\n",
    "Statement_Column = 'Statement'\n",
    "Numerical_Columns = [\n",
    "    'Credit History: barely-true', 'Credit History: false',\n",
    "    'Credit History: half-true', 'Credit History: mostly-true',\n",
    "    'Credit History: pants-fire'\n",
    "]\n",
    "Label_Column = 'Label'\n",
    "\n",
    "# batch size \n",
    "Batch_Size = 64\n",
    "\n",
    "# read file  \n",
    "Data_File = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "\n",
    "# preprocess the text \n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "natural_language_processor = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #Lemmatize every token in set, after tokenizaziotn \n",
    "    return [token.lemma_.lower() for token in natural_language_processor(str(text)) if not token.is_punct and not token.is_space and not token.is_stop and not token.is_alpha]\n",
    "\n",
    "# build the vocabulary\n",
    "word_count = Counter()\n",
    "for statement in Data_File[Statement_Column]:\n",
    "    word_count.update(tokenize_text(statement))\n",
    "\n",
    "# add PAD token to make same length and UNK (unkown for words not in the vocab)\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 50\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, count) in enumerate(word_count.most_common(MAX_VOCAB_SIZE- 2)): # -2 for pad and unk\n",
    "    vocab[word] = i + 2\n",
    "\n",
    "actual_vocab_size = len(vocab)\n",
    "\n",
    "# numericalize and pad/truncuate text\n",
    "def numericalize_pad_text(text, vocab_map, max_length):\n",
    "    tokens = tokenize_text(text)\n",
    "    numericalized = [vocab_map.get(token, vocab_map[UNK_TOKEN]) for token in tokens]\n",
    "    if len(numericalized) < max_length:\n",
    "        # Pad with PAD_TOKEN's index\n",
    "        numericalized.extend([vocab_map[PAD_TOKEN]] * (max_length - len(numericalized)))\n",
    "    else:\n",
    "        # Truncate\n",
    "        numericalized = numericalized[:max_length]\n",
    "    return numericalized\n",
    "\n",
    "# numericalize the statements into a new 'statement_numerical' column\n",
    "Data_File['statement_numerical'] = Data_File[Statement_Column].apply(lambda x: numericalize_pad_text(x, vocab, MAX_LENGTH))\n",
    "\n",
    "# map the labels to numerical values\n",
    "unique_labels = Data_File[Label_Column].astype(str).unique()\n",
    "label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# use mapping for data in new 'label_idx' column\n",
    "Data_File['label_idx'] = Data_File[Label_Column].astype(str).map(label_to_idx)\n",
    "\n",
    "for col in Numerical_Columns:\n",
    "    Data_File[col] = pd.to_numeric(Data_File[col], errors='coerce')\n",
    "    \n",
    "    if Data_File[col].isnull().any():\n",
    "        Data_File[col] = Data_File[col].fillna(0)\n",
    "\n",
    "# scale the credit history values\n",
    "# scaler = StandardScaler()\n",
    "# Data_File[Numerical_Columns] = scaler.fit_transform(Data_File[Numerical_Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aba92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LieDetectorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_lstm, num_numerical_features, hidden_dim_fc, output_dim, pad_idx):\n",
    "        super(LieDetectorModel, self).__init__()\n",
    "        # Embedding layer for text\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # LSTM layer to process sequential text data\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_lstm, batch_first=True, num_layers=1, bidirectional=True) # Using bidirectional LSTM\n",
    "      \n",
    "        lstm_output_features = hidden_dim_lstm * 2 \n",
    "\n",
    "        self.fc1 = nn.Linear(lstm_output_features + num_numerical_features, hidden_dim_fc)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        self.fc2 = nn.Linear(hidden_dim_fc, output_dim)\n",
    "\n",
    "    def forward(self, text_data, numerical_data):\n",
    "        embedded = self.embedding(text_data)     \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        combined_features = torch.cat((hidden_combined, numerical_data), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        # logits shape: (batch_size, output_dim)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (optim.Optimizer): The optimizer to use (e.g., Adam).\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        device (torch.device): The device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        #Go to training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            texts = batch['text'].to(device)\n",
    "            numerical_feats = batch['numerical'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(texts, numerical_feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * texts.size(0)\n",
    "\n",
    "            # calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples_train += labels.size(0)\n",
    "            correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = correct_predictions_train / total_samples_train\n",
    "\n",
    "        #Go to validation\n",
    "        model.eval() \n",
    "        running_val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "        #No grad for validation\n",
    "        with torch.no_grad(): \n",
    "            for batch in val_loader:\n",
    "                texts = batch['text'].to(device)\n",
    "                numerical_feats = batch['numerical'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(texts, numerical_feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * texts.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples_val += labels.size(0)\n",
    "                correct_predictions_val += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_acc_val = correct_predictions_val / total_samples_val\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc_train:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_val:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6528d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of numerical features being used: 5\n",
      "Starting training...\n",
      "Epoch [1/50] | Train Loss: 1.8945 | Train Acc: 0.1791 | Val Loss: 1.8417 | Val Acc: 0.1904\n",
      "Epoch [2/50] | Train Loss: 1.7931 | Train Acc: 0.2000 | Val Loss: 1.7537 | Val Acc: 0.2051\n",
      "Epoch [3/50] | Train Loss: 1.7699 | Train Acc: 0.2039 | Val Loss: 1.7458 | Val Acc: 0.2081\n",
      "Epoch [4/50] | Train Loss: 1.7582 | Train Acc: 0.2030 | Val Loss: 1.7416 | Val Acc: 0.2042\n",
      "Epoch [5/50] | Train Loss: 1.7490 | Train Acc: 0.2225 | Val Loss: 1.7401 | Val Acc: 0.2090\n",
      "Epoch [6/50] | Train Loss: 1.7418 | Train Acc: 0.2212 | Val Loss: 1.7337 | Val Acc: 0.2112\n",
      "Epoch [7/50] | Train Loss: 1.7354 | Train Acc: 0.2298 | Val Loss: 1.7320 | Val Acc: 0.2112\n",
      "Epoch [8/50] | Train Loss: 1.7331 | Train Acc: 0.2247 | Val Loss: 1.7300 | Val Acc: 0.2203\n",
      "Epoch [9/50] | Train Loss: 1.7291 | Train Acc: 0.2247 | Val Loss: 1.7279 | Val Acc: 0.2259\n",
      "Epoch [10/50] | Train Loss: 1.7239 | Train Acc: 0.2367 | Val Loss: 1.7270 | Val Acc: 0.2107\n",
      "Epoch [11/50] | Train Loss: 1.7187 | Train Acc: 0.2400 | Val Loss: 1.7258 | Val Acc: 0.2250\n",
      "Epoch [12/50] | Train Loss: 1.7166 | Train Acc: 0.2348 | Val Loss: 1.7239 | Val Acc: 0.2722\n",
      "Epoch [13/50] | Train Loss: 1.7131 | Train Acc: 0.2442 | Val Loss: 1.7229 | Val Acc: 0.2241\n",
      "Epoch [14/50] | Train Loss: 1.7089 | Train Acc: 0.2501 | Val Loss: 1.7213 | Val Acc: 0.2484\n",
      "Epoch [15/50] | Train Loss: 1.7077 | Train Acc: 0.2465 | Val Loss: 1.7224 | Val Acc: 0.2259\n",
      "Epoch [16/50] | Train Loss: 1.7015 | Train Acc: 0.2531 | Val Loss: 1.7213 | Val Acc: 0.2453\n",
      "Epoch [17/50] | Train Loss: 1.7012 | Train Acc: 0.2496 | Val Loss: 1.7208 | Val Acc: 0.2756\n",
      "Epoch [18/50] | Train Loss: 1.6961 | Train Acc: 0.2547 | Val Loss: 1.7226 | Val Acc: 0.2306\n",
      "Epoch [19/50] | Train Loss: 1.6956 | Train Acc: 0.2582 | Val Loss: 1.7207 | Val Acc: 0.2462\n",
      "Epoch [20/50] | Train Loss: 1.6908 | Train Acc: 0.2602 | Val Loss: 1.7200 | Val Acc: 0.2453\n",
      "Epoch [21/50] | Train Loss: 1.6911 | Train Acc: 0.2601 | Val Loss: 1.7235 | Val Acc: 0.2406\n",
      "Epoch [22/50] | Train Loss: 1.6889 | Train Acc: 0.2640 | Val Loss: 1.7217 | Val Acc: 0.2505\n",
      "Epoch [23/50] | Train Loss: 1.6888 | Train Acc: 0.2630 | Val Loss: 1.7220 | Val Acc: 0.2441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/starmind_hack_2025/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/starmind_hack_2025/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mLieDetectorModel.forward\u001b[0;34m(self, text_data, numerical_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_data, numerical_data):\n\u001b[1;32m     17\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(text_data)     \n\u001b[0;32m---> 18\u001b[0m     lstm_out, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     hidden_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:], hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden_combined, numerical_data), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/starmind_hack_2025/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/starmind_hack_2025/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/starmind_hack_2025/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "# split up the data in the validation and training set\n",
    "Train_df, Val_df = train_test_split(Data_File, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset for statements (processing textual data into numerical data)\n",
    "class TextNumericalDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_col_numerical, numerical_cols_list, label_col_idx):\n",
    "        self.texts = torch.tensor(list(dataframe[text_col_numerical].values), dtype=torch.long)\n",
    "        self.numerical_features = torch.tensor(dataframe[numerical_cols_list].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataframe[label_col_idx].values, dtype=torch.long) # Assuming classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'numerical': self.numerical_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextNumericalDataset(Train_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "val_dataset = TextNumericalDataset(Val_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "\n",
    "# data loader for both datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Batch_Size, shuffle=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # determine device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    num_numerical_features = train_dataset.numerical_features.shape[1]\n",
    "    print(f\"Number of numerical features being used: {num_numerical_features}\")\n",
    "    pad_idx = vocab[PAD_TOKEN]\n",
    "    model = LieDetectorModel(\n",
    "        vocab_size=actual_vocab_size,\n",
    "        embedding_dim=100,\n",
    "        hidden_dim_lstm=64,\n",
    "        num_numerical_features=num_numerical_features,\n",
    "        hidden_dim_fc=128,\n",
    "        output_dim=num_classes,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)  # Using AdamW optimizer\n",
    "    # CrossEntropyLoss is suitable for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # start training\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, NUM_EPOCHS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
