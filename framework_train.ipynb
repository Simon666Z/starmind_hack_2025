{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch import nn, tensor, save, load\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for numerical features\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LieDetectorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_lstm, num_numerical_features, hidden_dim_fc, output_dim, pad_idx):\n",
    "        super(LieDetectorModel, self).__init__()\n",
    "        # Embedding layer for text\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # LSTM layer to process sequential text data\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_lstm, batch_first=True, num_layers=1, bidirectional=True) # Using bidirectional LSTM\n",
    "      \n",
    "        lstm_output_features = hidden_dim_lstm * 2 \n",
    "\n",
    "        self.fc1 = nn.Linear(lstm_output_features + num_numerical_features, hidden_dim_fc)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        self.fc2 = nn.Linear(hidden_dim_fc, output_dim)\n",
    "\n",
    "    def forward(self, text_data, numerical_data):\n",
    "        embedded = self.embedding(text_data)     \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        combined_features = torch.cat((hidden_combined, numerical_data), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        # logits shape: (batch_size, output_dim)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (optim.Optimizer): The optimizer to use (e.g., Adam).\n",
    "        criterion (nn.Module): The loss function (e.g., CrossEntropyLoss).\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        device (torch.device): The device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        #Go to training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            texts = batch['text'].to(device)\n",
    "            numerical_feats = batch['numerical'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            #Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Forward pass\n",
    "            outputs = model(texts, numerical_feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * texts.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples_train += labels.size(0)\n",
    "            correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = correct_predictions_train / total_samples_train\n",
    "\n",
    "        #Go to validation\n",
    "        model.eval() \n",
    "        running_val_loss = 0.0\n",
    "        correct_predictions_val = 0\n",
    "        total_samples_val = 0\n",
    "        #No grad for validation\n",
    "        with torch.no_grad(): \n",
    "            for batch in val_loader:\n",
    "                texts = batch['text'].to(device)\n",
    "                numerical_feats = batch['numerical'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(texts, numerical_feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * texts.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples_val += labels.size(0)\n",
    "                correct_predictions_val += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_acc_val = correct_predictions_val / total_samples_val\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc_train:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_val:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Determine device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    num_numerical_features = train_dataset.numerical_features.shape[1]\n",
    "    print(f\"Number of numerical features being used: {num_numerical_features}\")\n",
    "    pad_idx = vocab[PAD_TOKEN]\n",
    "    model = SimpleTextNN(\n",
    "        vocab_size=actual_Vocab_Size,\n",
    "        embedding_dim=Embedding_Dim,\n",
    "        hidden_dim_lstm=Hidden_Dim_LSTM,\n",
    "        num_numerical_features=num_numerical_features,\n",
    "        hidden_dim_fc=Hidden_Dim_FC,\n",
    "        output_dim=num_classes,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Learning_Rate)\n",
    "    # CrossEntropyLoss is suitable for multi-class classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, optimizer, criterion, Num_Epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for numerical features\n",
    "import numpy as np\n",
    "\n",
    "File_Path = 'eestec_hackathon_2025_train.tsv'\n",
    "Text_Column = 'Statement'\n",
    "Numerical_Columns = [\n",
    "    'Credit History: barely-true', 'Credit History: false',\n",
    "    'Credit History: half-true', 'Credit History: mostly-true',\n",
    "    'Credit History: pants-fire'\n",
    "]\n",
    "Label_Column = 'Label'\n",
    "\n",
    "\n",
    "#Batch size \n",
    "Batch_Size = 64\n",
    "\n",
    "#Read file  \n",
    "Data_File = pd.read_csv('eestec_hackathon_2025_train.tsv' ,sep = '\\t',names=['ID', 'Label', 'Statement', 'Subjects', 'Speaker Name', 'Speaker Title', 'State', 'Party Affiliation', 'Credit History: barely-true', 'Credit History: false', 'Credit History: half-true', 'Credit History: mostly-true', 'Credit History: pants-fire', 'Context/Location'])\n",
    "\n",
    "#Split up the data in the validation and training set\n",
    "Train_df, Val_df = train_test_split(Data_File, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the text \n",
    "natural_language_processor = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #Lemmatize every token in set, after tokenizaziotn \n",
    "    return [token.lemma_.lower() for token in natural_language_processor(str(text)) if not token.is_punct and not token.is_space]\n",
    "\n",
    "#Now we build the vocabulary \n",
    "\n",
    "word_count = Counter()\n",
    "for statement in Train_df[Text_Column]:\n",
    "    word_count.update(tokenize_text(statement))\n",
    "\n",
    "# add PAD token to make same length and UNK(unkown for words not in the vocab)\n",
    "Max_Vocab_Size = 10000\n",
    "Max_Length = 50  \n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, count) in enumerate(word_count.most_common(Max_Vocab_Size- 2)): # -2 for pad and unk\n",
    "    vocab[word] = i + 2\n",
    "\n",
    "actual_Vocab_Size = len[vocab]\n",
    "\n",
    "#Numericalize and pad/truncuate text\n",
    "def numericalize_pad_text(text, vocab_map, max_len):\n",
    "    tokens = tokenize_text(text)\n",
    "    numericalized = [vocab_map.get(token, vocab_map[UNK_TOKEN]) for token in tokens]\n",
    "    if len(numericalized) < max_len:\n",
    "        # Pad with PAD_TOKEN's index\n",
    "        numericalized.extend([vocab_map[PAD_TOKEN]] * (max_len - len(numericalized)))\n",
    "    else:\n",
    "        # Truncate\n",
    "        numericalized = numericalized[:max_len]\n",
    "    return numericalized\n",
    "\n",
    "# Apply to Statement column\n",
    "Train_df['statement_numerical'] = Train_df[Text_Column].apply(lambda x: numericalize_pad_text(x, vocab, Max_Length))\n",
    "Val_df['statement_numerical'] = Val_df[Text_Column].apply(lambda x: numericalize_pad_text(x, vocab, Max_Length))\n",
    "\n",
    "#Later maybe use scalar?\n",
    "#scaler = StandardScaler()\n",
    "#Train_df[Numerical_Columns] = scaler.fit_transform(Train_df[Numerical_Columns])\n",
    "#Val_df[Numerical_Columns] = scaler.transform(Val_df[Numerical_Columns]) # Use transform for validation\n",
    "\n",
    "# Mapping the labels to numbers \n",
    "unique_labels = Train_df[Label_Column].astype(str).unique()\n",
    "label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "#Use mapping for data \n",
    "Train_df['label_idx'] = Train_df[Label_Column].astype(str).map(label_to_idx)\n",
    "Val_df['label_idx'] = Val_df[Label_Column].astype(str).map(label_to_idx)\n",
    "\n",
    "#Verify please \n",
    "\n",
    "class TextNumericalDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_col_numerical, numerical_cols_list, label_col_idx):\n",
    "        self.texts = torch.tensor(list(dataframe[text_col_numerical].values), dtype=torch.long)\n",
    "        self.numerical_features = torch.tensor(dataframe[numerical_cols_list].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataframe[label_col_idx].values, dtype=torch.long) # Assuming classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'numerical': self.numerical_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextNumericalDataset(Train_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "val_dataset = TextNumericalDataset(Val_df, 'statement_numerical', Numerical_Columns, 'label_idx')\n",
    "\n",
    "#Data loader \n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Batch_Size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
